## Deep Distributionally Robust Learning for Calibrated Uncertainties under Domain Shift

### Introduction

 We propose a deep distributionally robust learning (DRL) method for generating calibrated uncertainties for modern computer vision tasks. We consider domain shift, where the source distribution for training differs from the target distribution for testing. Our framework involves two neural networks that are jointly trained --- a domain classifier between the  source and target domains  for density-ratio estimation, and the standard target classifier for the learning tasks. We demonstrate that this method generates calibrated uncertainties for many downstream tasks. For instance, unsupervised domain adaptation (UDA) and semi-supervised learning (SSL) typically require uncertainty-based selection of pseudo-labels. We use the uncertainty estimates generated by deep DRL to select confident pseudo-labels. In our experiments, DRL-powered UDA achieves state-of-the-art performance on benchmark datasets. In addition, DRL-powered SSL significantly improves cross-domain performance over baselines. We also demonstrate that our density ratio estimates match the human selection frequencies, meaning they reflect the hardness of an image, as perceived by humans.

#### Environment

* Python 3.6
* PyTorch 0.4.0
* CUDA version 9.0
(This combination of environment can be obtained by using docker images)

#### File introduction

We provide the introduction to the folders and files in this section
* model_layers.py defines the foundations of DRL, DRST and DRSSL models;
* office_exp.py contains the code for running experiments on Office31 and Office-Home;
* visda_exp.py contained the DRST training code for VisDA 2017 dataset;
* tempertaure_scaling.py contains the code for implementing TS; 
* FixMatch-pytorch-master contains the SSL training code. train.py contains the code for achieving pretrained models, while train_var.py contains the code for DRSSL;
* imagenet_train.py contains the code for training on ImageNet using DRL;
* Other files are included for plotting and discussion.


### Code Usage


The training code is generally written in a manner easy for debugging and experimenting on different tasks. Here we provide the example scripts for specific tasks.

#### Office31 and Office-Home:

For Office31 and OfficeHome, the two datasets are trained in a quite similar way, thus the only difference is in the code are the data paths needed and the 

Take the one of the tasks in the Office31 dataset as example, the code can be run by directly using:

```
python office_exp.py --num_classes=31 --src=amazon --tgt==webcam
```

When changed to OfficeHome dataset, the users can change the number of classes into 65 and assign the **path_dict** new values with the intended path. Do not forget to write new paths for the saved
models and metrics to avoid overwrite.

#### VisDA 2017

For VisDA 2017, we adopt the same training criterion as CBST. The procedure is included in visda_exp.py. In the code, we include both the 
original CBST (train_and_val_cbst()) and our DRL training (train_and_val_rescue_var3()) procedure, so that the users can compare their differences.

```
python visda_exp.py
```

The specific hyperparameters in the code can be changed via the **CONFIG** parameter, which is included at the top of the file. We set the random seed to 5 for better performance. 

#### ImageNet 

ImageNet training process is directly modified from the official PyTorch training script. The users can directly run the script and achieve corresponding calibration scores. Note that 
the code is not written in a parallelism way so batch size is set to a small value and the users need to make sure they have abundant computation resources before running it. Code can be 
run by:

```
python imagenet_train.py
```

#### Note

For other code (not for training ones, the users may need to select the functions they need and set the specific path for saved models)