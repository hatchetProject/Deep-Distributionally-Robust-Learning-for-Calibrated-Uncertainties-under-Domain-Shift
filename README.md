## Deep Distributionally Robust Learning for CalibratedUncertainties under Domain Shift

### Introduction

 We propose a deep distributionally robust learning (DRL) method for generating calibrated uncertainties for modern computer vision tasks. We consider domain shift, where the source distribution for training differs from the target distribution for testing. Our framework involves two neural networks that are jointly trained --- a domain classifier between the  source and target domains  for density-ratio estimation, and the standard target classifier for the learning tasks. We demonstrate that this method generates calibrated uncertainties for many downstream tasks. For instance, unsupervised domain adaptation (UDA) and semi-supervised learning (SSL) typically require uncertainty-based selection of pseudo-labels. We use the uncertainty estimates generated by deep DRL to select confident pseudo-labels. In our experiments, DRL-powered UDA achieves state-of-the-art performance on benchmark datasets. In addition, DRL-powered SSL significantly improves cross-domain performance over baselines. We also demonstrate that our density ratio estimates match the human selection frequencies, meaning they reflect the hardness of an image, as perceived by humans.

#### Environment

* Python 3.6
* PyTorch 0.4.0
* CUDA version 9.0

#### File introduction

We provide the introduction to the folders and files in this section
* model_layers.py defines the foundations of DRL, DRST and DRSSL models;
* visda_exp.py contained the DRST training code for VisDA 2017 dataset;
* 
* FixMatch-pytorch-master contains the SSL training code. train.py contains the code for achieving pretrained models, while train_var.py contains the code for DRSSL.

